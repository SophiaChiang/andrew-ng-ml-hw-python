{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In which the contents of numerous spam folders gradually erodes my faith in humanity. \n",
    "\n",
    "Week 7 of Andrew Ng's ML course on Coursera introduces the Support Vector Machine algorithm and challenges us to use it for classifying email as spam or ham. Here I use the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to build an SVM spam email classifier in order to learn about the relevant python tools. Part I focuses on the preprocessing of individual emails while Part II focuses on the actual classifier.\n",
    "\n",
    ">## Tools Covered:\n",
    "- `re` for regular expressions to do Natural Language Processing (NLP)\n",
    "- `stopwords` text corpus for removing information-poor words in NLP\n",
    "- `SnowballStemmer` for stemming text in NLP\n",
    "- `BeautifulSoup` for HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import snips as snp  # my snippets\n",
    "snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonya\\Box Sync\\Projects\\course-machine-learning\\hw-wk7-spam-preprocessing\n"
     ]
    }
   ],
   "source": [
    "cd hw-wk7-spam-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at the Data\n",
    "\n",
    "I'm going to pull a set of spam and \"ham\" (non-spam) emails from the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) data sets. This resource has also kindly ham separated emails into easy and hard ham. Each email is stored a a plain text file with the email header information and the email body including HTML markup if applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for accessing all the spam and ham text files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "spampath = join(os.getcwd(), \"spam\")\n",
    "spamfiles = [join(spampath, fname) for fname in listdir(spampath)]\n",
    "\n",
    "hampath = join(os.getcwd(), \"easy_ham\")\n",
    "hamfiles = [join(hampath, fname) for fname in listdir(hampath)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Formatted  File\n",
    "Here is what an email would look like if viewed with proper formatting, like in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From hghwebmaster@Flashmail.com  Mon Sep  2 12:18:25 2002\n",
      "\n",
      "Return-Path: <hghwebmaster@Flashmail.com>\n",
      "\n",
      "Delivered-To: zzzz@localhost.example.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.example.com (Postfix) with ESMTP id E145844157\n",
      "\n",
      "\tfor <zzzz@localhost>; Mon,  2 Sep 2002 07:18:19 -0400 (EDT)\n",
      "\n",
      "Received: from mail.webnote.net [193.120.211.219]\n",
      "\n",
      "\tby localhost with POP3 (fetchmail-5.9.0)\n",
      "\n",
      "\tfor zzzz@localhost (single-drop); Mon, 02 Sep 2002 12:18:19 +0100 (IST)\n",
      "\n",
      "Received: from main.goldenhorse.com.cn ([61.151.251.117])\n",
      "\n",
      "\tby webnote.net (8.9.3/8.9.3) with ESMTP id AAA15656\n",
      "\n",
      "\tfor <zzzz@example.com>; Sat, 31 Aug 2002 00:09:23 +0100\n",
      "\n",
      "Received: from smtp0943.mail.yahoo.com ([218.24.129.171])\n",
      "\n",
      "\tby main.goldenhorse.com.cn (AIX4.3/8.9.3/8.9.3) with SMTP id HAA215120;\n",
      "\n",
      "\tSat, 31 Aug 2002 07:06:31 +0800\n",
      "\n",
      "Message-Id: <200208302306.HAA215120@main.goldenhorse.com.cn>\n",
      "\n",
      "Date: Sat, 31 Aug 2002 07:07:19 +0800\n",
      "\n",
      "From: \"Raymond Feyl\" <hghwebmaster@Flashmail.com>\n",
      "\n",
      "X-Priority: 3\n",
      "\n",
      "To: zzzz@neilgarner.com\n",
      "\n",
      "Cc: zzzz@neo.lrun.com, jm@neo.rr.com, jm@netmore.net, jm@example.com\n",
      "\n",
      "Subject: zzzz,Your HGH Request!\n",
      "\n",
      "Mime-Version: 1.0\n",
      "\n",
      "Content-Transfer-Encoding: 7bit\n",
      "\n",
      "Content-Type: text/html; charset=us-ascii\n",
      "\n",
      "\n",
      "\n",
      "<html>\n",
      "\n",
      "<body bgColor=\"#CCCCCC\" topmargin=1 onMouseOver=\"window.status=''; return true\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart=\"return false\">\n",
      "\n",
      "Hello, zzzz@neilgarner.com<BR>\n",
      "\n",
      "<BR>\n",
      "\n",
      "As se<!--5-->en on NB<!--D-->C, CBS, and CN<!--H-->N, and even Opr<!--D-->ah!  The health<br>\n",
      "\n",
      "discove<!--F-->ry that actually revers<!--D-->es aging while burning fat,<br>\n",
      "\n",
      "with<!--boy-->out dieti<!--D-->ng or exerc<!--F-->ise!  This pro<!--A-->ven discovery has even<br>\n",
      "\n",
      "been report<!--zzzz-->ed on by the Ne<!--test-->w Engl<!---->and Jour<!--F-->nal of Medi<!--F-->cine.<br>\n",
      "\n",
      "For<!--zzzz-->get aging and d<!---->ieting forever!  And it's Gua<!--S-->ranteed!<br>\n",
      "\n",
      "<br><br>\n",
      "\n",
      "* Red<!--lo-->uce body fat and build lean muscle WIT<!--zzzz-->HOUT EXERCISE!<br>\n",
      "\n",
      "* Enha<!--zzzz-->ce se<!--la-->xual perf<!--hehe-->ormance<br>\n",
      "\n",
      "* Rem<!--zzzz-->ove wrinkles and cellulite<br>\n",
      "\n",
      "* Lower blood pres<!--zzzz-->sure and improve choles<!---->terol profile<br>\n",
      "\n",
      "* Imp<!--zzzz-->rove sleep, vision and me<!---->mory<br>\n",
      "\n",
      "* Resto<!--zzzz-->re hair color and gro<!---->wth<br>\n",
      "\n",
      "* Stren<!--zzzz-->gthen the immune sys<!---->tem<br>\n",
      "\n",
      "* Incre<!--zzzz-->ase ener<!---->gy and card<!---->iac output<br>\n",
      "\n",
      "* Turn bac<!--zzzz-->k your body's biol<!---->ogical time cl<!---->ock 10-20 years<br>\n",
      "\n",
      "in 6 months of usage !!!<br><br>\n",
      "\n",
      "<a href=\"http://202.101.163.34:81/ultimatehgh_zeng/index.html\">FOR FRE<!--o-->E INFO<!--you-->RMATION AND G<!--love-->ET FREE \n",
      "\n",
      "1 MON<!--zzzz-->TH SUPPLY OF HG<!---->H CLICK HERE</a><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR>\n",
      "\n",
      "You are recei<!--zzzz-->ving this email as a subscr<!---->iber<br>\n",
      "\n",
      "to the Opt<!--zzzz-->-In Ameri<!---->ca Mailin<!---->g Lis<!---->t. <br>\n",
      "\n",
      "To remo<!--zzzz-->ve your<!---->self from all related mailli<!--me-->sts,<br>\n",
      "\n",
      "just <a href=\"http://202.101.163.34:81/ultimatehgh_zeng/remove.php?userid=zzzz@neilgarner.com\"> \n",
      "\n",
      "Click Here</a>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(spamfiles[179]) as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Raw File\n",
    "Now we want to see what the actual strings look like that we will do all our processing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From hghwebmaster@Flashmail.com  Mon Sep  2 12:18:25 2002\\n', 'Return-Path: <hghwebmaster@Flashmail.com>\\n', 'Delivered-To: zzzz@localhost.example.com\\n', 'Received: from localhost (localhost [127.0.0.1])\\n', '\\tby phobos.labs.example.com (Postfix) with ESMTP id E145844157\\n', '\\tfor <zzzz@localhost>; Mon,  2 Sep 2002 07:18:19 -0400 (EDT)\\n', 'Received: from mail.webnote.net [193.120.211.219]\\n', '\\tby localhost with POP3 (fetchmail-5.9.0)\\n', '\\tfor zzzz@localhost (single-drop); Mon, 02 Sep 2002 12:18:19 +0100 (IST)\\n', 'Received: from main.goldenhorse.com.cn ([61.151.251.117])\\n', '\\tby webnote.net (8.9.3/8.9.3) with ESMTP id AAA15656\\n', '\\tfor <zzzz@example.com>; Sat, 31 Aug 2002 00:09:23 +0100\\n', 'Received: from smtp0943.mail.yahoo.com ([218.24.129.171])\\n', '\\tby main.goldenhorse.com.cn (AIX4.3/8.9.3/8.9.3) with SMTP id HAA215120;\\n', '\\tSat, 31 Aug 2002 07:06:31 +0800\\n', 'Message-Id: <200208302306.HAA215120@main.goldenhorse.com.cn>\\n', 'Date: Sat, 31 Aug 2002 07:07:19 +0800\\n', 'From: \"Raymond Feyl\" <hghwebmaster@Flashmail.com>\\n', 'X-Priority: 3\\n', 'To: zzzz@neilgarner.com\\n', 'Cc: zzzz@neo.lrun.com, jm@neo.rr.com, jm@netmore.net, jm@example.com\\n', 'Subject: zzzz,Your HGH Request!\\n', 'Mime-Version: 1.0\\n', 'Content-Transfer-Encoding: 7bit\\n', 'Content-Type: text/html; charset=us-ascii\\n', '\\n', '<html>\\n', '<body bgColor=\"#CCCCCC\" topmargin=1 onMouseOver=\"window.status=\\'\\'; return true\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart=\"return false\">\\n', 'Hello, zzzz@neilgarner.com<BR>\\n', '<BR>\\n', 'As se<!--5-->en on NB<!--D-->C, CBS, and CN<!--H-->N, and even Opr<!--D-->ah!  The health<br>\\n', 'discove<!--F-->ry that actually revers<!--D-->es aging while burning fat,<br>\\n', 'with<!--boy-->out dieti<!--D-->ng or exerc<!--F-->ise!  This pro<!--A-->ven discovery has even<br>\\n', 'been report<!--zzzz-->ed on by the Ne<!--test-->w Engl<!---->and Jour<!--F-->nal of Medi<!--F-->cine.<br>\\n', \"For<!--zzzz-->get aging and d<!---->ieting forever!  And it's Gua<!--S-->ranteed!<br>\\n\", '<br><br>\\n', '* Red<!--lo-->uce body fat and build lean muscle WIT<!--zzzz-->HOUT EXERCISE!<br>\\n', '* Enha<!--zzzz-->ce se<!--la-->xual perf<!--hehe-->ormance<br>\\n', '* Rem<!--zzzz-->ove wrinkles and cellulite<br>\\n', '* Lower blood pres<!--zzzz-->sure and improve choles<!---->terol profile<br>\\n', '* Imp<!--zzzz-->rove sleep, vision and me<!---->mory<br>\\n', '* Resto<!--zzzz-->re hair color and gro<!---->wth<br>\\n', '* Stren<!--zzzz-->gthen the immune sys<!---->tem<br>\\n', '* Incre<!--zzzz-->ase ener<!---->gy and card<!---->iac output<br>\\n', \"* Turn bac<!--zzzz-->k your body's biol<!---->ogical time cl<!---->ock 10-20 years<br>\\n\", 'in 6 months of usage !!!<br><br>\\n', '<a href=\"http://202.101.163.34:81/ultimatehgh_zeng/index.html\">FOR FRE<!--o-->E INFO<!--you-->RMATION AND G<!--love-->ET FREE \\n', '1 MON<!--zzzz-->TH SUPPLY OF HG<!---->H CLICK HERE</a><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR>\\n', 'You are recei<!--zzzz-->ving this email as a subscr<!---->iber<br>\\n', 'to the Opt<!--zzzz-->-In Ameri<!---->ca Mailin<!---->g Lis<!---->t. <br>\\n', 'To remo<!--zzzz-->ve your<!---->self from all related mailli<!--me-->sts,<br>\\n', 'just <a href=\"http://202.101.163.34:81/ultimatehgh_zeng/remove.php?userid=zzzz@neilgarner.com\"> \\n', 'Click Here</a>\\n', '</body>\\n', '</html>\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(spamfiles[179], \"r\") as myfile:\n",
    "    lines = myfile.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary thoughts: The first line of every file is the most basic header info about the originating address and time the email was sent. There follows a section of keyword-value pairs in the form *keyword: value\\n*. Finally, **the body of each email is separated from the meta info by two newline characters `\\n\\n`.** Note that some of the email bodies contain HTML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Mode with Just the Email Body\n",
    "The first thing I'll try is just doing some NLP on only the email bodies, ignoring all the header info. First write a function that grabs only the body lines of a single email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(fpath):\n",
    "    '''Get email body lines from fpath using first occurence of empty line.'''\n",
    "    with open(fpath, \"r\") as myfile:\n",
    "        try: \n",
    "            lines = myfile.readlines()\n",
    "            idx = lines.index(\"\\n\") # only grabs first instance\n",
    "            return \"\".join(lines[idx:])\n",
    "        except: \n",
    "            print(\"Couldn't decode file %s\" %(fpath,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<html>\\n<body bgColor=\"#CCCCCC\" topmargin=1 onMouseOver=\"window.status=\\'\\'; return true\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart=\"return false\">\\nHello, zzzz@neilgarner.com<BR>\\n<BR>\\nAs se<!--5-->en on NB<!--D-->C, CBS, and CN<!--H-->N, and even Opr<!--D-->ah!  The health<br>\\ndiscove<!--F-->ry that actually revers<!--D-->es aging while burning fat,<br>\\nwith<!--boy-->out dieti<!--D-->ng or exerc<!--F-->ise!  This pro<!--A-->ven discovery has even<br>\\nbeen report<!--zzzz-->ed on by the Ne<!--test-->w Engl<!---->and Jour<!--F-->nal of Medi<!--F-->cine.<br>\\nFor<!--zzzz-->get aging and d<!---->ieting forever!  And it\\'s Gua<!--S-->ranteed!<br>\\n<br><br>\\n* Red<!--lo-->uce body fat and build lean muscle WIT<!--zzzz-->HOUT EXERCISE!<br>\\n* Enha<!--zzzz-->ce se<!--la-->xual perf<!--hehe-->ormance<br>\\n* Rem<!--zzzz-->ove wrinkles and cellulite<br>\\n* Lower blood pres<!--zzzz-->sure and improve choles<!---->terol profile<br>\\n* Imp<!--zzzz-->rove sleep, vision and me<!---->mory<br>\\n* Resto<!--zzzz-->re hair color and gro<!---->wth<br>\\n* Stren<!--zzzz-->gthen the immune sys<!---->tem<br>\\n* Incre<!--zzzz-->ase ener<!---->gy and card<!---->iac output<br>\\n* Turn bac<!--zzzz-->k your body\\'s biol<!---->ogical time cl<!---->ock 10-20 years<br>\\nin 6 months of usage !!!<br><br>\\n<a href=\"http://202.101.163.34:81/ultimatehgh_zeng/index.html\">FOR FRE<!--o-->E INFO<!--you-->RMATION AND G<!--love-->ET FREE \\n1 MON<!--zzzz-->TH SUPPLY OF HG<!---->H CLICK HERE</a><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR><br><BR>\\nYou are recei<!--zzzz-->ving this email as a subscr<!---->iber<br>\\nto the Opt<!--zzzz-->-In Ameri<!---->ca Mailin<!---->g Lis<!---->t. <br>\\nTo remo<!--zzzz-->ve your<!---->self from all related mailli<!--me-->sts,<br>\\njust <a href=\"http://202.101.163.34:81/ultimatehgh_zeng/remove.php?userid=zzzz@neilgarner.com\"> \\nClick Here</a>\\n</body>\\n</html>\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out \n",
    "body= get_body(join(hampath, spamfiles[179]))\n",
    "# body= get_body(spamfiles[30])\n",
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Plan of Attack (order matters)\n",
    "The order of steps in text processing matters a lot if you are trying to extract other features alongside a simple \"Bag of Words\" or \"Word Salad\" model. For instance, if you want to count the number of question marks in the email text then you should probably do it *before* removing all punctuation, but *after* replacing all http addresses (which sometimes contain special characters). Here is a rough outline of all the steps we'll take to get from a messy, marked-up raw text to a delicious word salad:\n",
    "- Strip any HTML tags and leave only text content (also count HTML tags)\n",
    "- Strip all email and web addresses (also count them)\n",
    "- Lowercase everything (also count uppercases)\n",
    "- Strip all dollar signs and numbers(also count them)\n",
    "- Strip away all other punctuation (also count exclamation and question marks)\n",
    "- Standardize all white space to single space (also count newlines and blank lines)\n",
    "- Count the total number of words in our word salad\n",
    "- Strip away all useless \"Stopwords\" (like \"a\", \"the\", \"at\")\n",
    "- Stem all the words down to their root to simplify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML\n",
    "Some of the email bodies contain HTML formatting - the amount of such formatting might be a helpful feature, but the tags themselves we want to strip away. There are also some symbols in HTML documents, like \"<\", that have a reserved shorthand notation since they are otherwise interpreted as markup by the browser. We could write regexes to do all of this HTML processing, but a lovely little package called `beatiful soup` has already done this and provided us with an HTML parser that returns a parsed object. The `get_text()` method lets us pull out everything *except* the markup from this parsed object. You should check out the [official soup docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the email body into HTML elements\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(body, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of HTML elements and specific link elements\n",
    "nhtml = len(soup.find_all())\n",
    "nlinks = len(soup.find_all(\"a\"))\n",
    "\n",
    "# Pull out only the non-markup of the body\n",
    "body = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nHello, zzzz@neilgarner.com\\n\\nAs seen on NBC, CBS, and CNN, and even Oprah!  The health\\ndiscovery that actually reverses aging while burning fat,\\nwithout dieting or exercise!  This proven discovery has even\\nbeen reported on by the New England Journal of Medicine.\\nForget aging and dieting forever!  And it's Guaranteed!\\n\\n* Reduce body fat and build lean muscle WITHOUT EXERCISE!\\n* Enhace sexual performance\\n* Remove wrinkles and cellulite\\n* Lower blood pressure and improve cholesterol profile\\n* Improve sleep, vision and memory\\n* Restore hair color and growth\\n* Strengthen the immune system\\n* Increase energy and cardiac output\\n* Turn back your body's biological time clock 10-20 years\\nin 6 months of usage !!!\\nFOR FREE INFORMATION AND GET FREE \\n1 MONTH SUPPLY OF HGH CLICK HERE\\nYou are receiving this email as a subscriber\\nto the Opt-In America Mailing List. \\nTo remove yourself from all related maillists,\\njust  \\nClick Here\\n\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding email and web addresses\n",
    "We'll find and count the appearances of email and web addresses, and then replace each one with blank space. A very useful tool for all language processing is the **regular expression**, which is housed in the `re` module of the python standard lib. For more info you can refer to my brief but hopefully edifying [overview of regexes in python](http://sdsawtelle.github.io/blog/output/regular-expressions-in-python.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace and count all URLs \n",
    "regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "body, nhttps = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Replace and count all email addresses\n",
    "regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "body, nemails = regx.subn(repl=\" \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nHello,  \\n\\nAs seen on NBC, CBS, and CNN, and even Oprah!  The health\\ndiscovery that actually reverses aging while burning fat,\\nwithout dieting or exercise!  This proven discovery has even\\nbeen reported on by the New England Journal of Medicine.\\nForget aging and dieting forever!  And it's Guaranteed!\\n\\n* Reduce body fat and build lean muscle WITHOUT EXERCISE!\\n* Enhace sexual performance\\n* Remove wrinkles and cellulite\\n* Lower blood pressure and improve cholesterol profile\\n* Improve sleep, vision and memory\\n* Restore hair color and growth\\n* Strengthen the immune system\\n* Increase energy and cardiac output\\n* Turn back your body's biological time clock 10-20 years\\nin 6 months of usage !!!\\nFOR FREE INFORMATION AND GET FREE \\n1 MONTH SUPPLY OF HGH CLICK HERE\\nYou are receiving this email as a subscriber\\nto the Opt-In America Mailing List. \\nTo remove yourself from all related maillists,\\njust  \\nClick Here\\n\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing and Counting Caps\n",
    "We don't expect whether a word is capitalized or not to reflect some deep difference in tone or meaning, but we *might* expect that an email with a bunch of capitalization reflects a certain tone, so we'll lowercase everything but still count the number of capitalized letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count uppercases\n",
    "nupper = len([charup for charup, char in zip(body, body.lower()) if charup != char])\n",
    "# Lowercase everything\n",
    "body = body.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nhello,  \\n\\nas seen on nbc, cbs, and cnn, and even oprah!  the health\\ndiscovery that actually reverses aging while burning fat,\\nwithout dieting or exercise!  this proven discovery has even\\nbeen reported on by the new england journal of medicine.\\nforget aging and dieting forever!  and it's guaranteed!\\n\\n* reduce body fat and build lean muscle without exercise!\\n* enhace sexual performance\\n* remove wrinkles and cellulite\\n* lower blood pressure and improve cholesterol profile\\n* improve sleep, vision and memory\\n* restore hair color and growth\\n* strengthen the immune system\\n* increase energy and cardiac output\\n* turn back your body's biological time clock 10-20 years\\nin 6 months of usage !!!\\nfor free information and get free \\n1 month supply of hgh click here\\nyou are receiving this email as a subscriber\\nto the opt-in america mailing list. \\nto remove yourself from all related maillists,\\njust  \\nclick here\\n\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding numbers, dollar signs, and punctuation\n",
    "We'd like to know the frequency of punctuation which carry certain tones, like exclamation marks, question marks, and dollar signs. Also the frequency of numbers appearing in the email might be a helpful feature. All of these frequencies should be normalized to the number of words in the email to measure the tone or intent of the email rather than its length, but we'll hold off on word-count until we're done with processing. After counting the things we care about, we'll remove all punctuation to get us closer to a pure bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count and replace all numbers (integer and float)\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "body, nnum = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Count and replace all dollar signs\n",
    "regx = re.compile(r\"[$]\")\n",
    "body, ndollar = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Count number of special punctuation\n",
    "nexclaim, nquest = body.count(\"!\"), body.count(\"?\")\n",
    "\n",
    "# Remove all other punctuation (dashes replace with space)\n",
    "regx = re.compile(r\"[^\\w\\s_-]+\")  \n",
    "body = regx.sub(repl=\"\", string=body)\n",
    "regx = re.compile(r\"[_-]+\")\n",
    "body = regx.sub(repl=\" \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nhello  \\n\\nas seen on nbc cbs and cnn and even oprah  the health\\ndiscovery that actually reverses aging while burning fat\\nwithout dieting or exercise  this proven discovery has even\\nbeen reported on by the new england journal of medicine\\nforget aging and dieting forever  and its guaranteed\\n\\n reduce body fat and build lean muscle without exercise\\n enhace sexual performance\\n remove wrinkles and cellulite\\n lower blood pressure and improve cholesterol profile\\n improve sleep vision and memory\\n restore hair color and growth\\n strengthen the immune system\\n increase energy and cardiac output\\n turn back your bodys biological time clock     years\\nin   months of usage \\nfor free information and get free \\n  month supply of hgh click here\\nyou are receiving this email as a subscriber\\nto the opt in america mailing list \\nto remove yourself from all related maillists\\njust  \\nclick here\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing White Space and Total Word Count\n",
    "Standardizing white space is an important step, as it makes tokenizing the email into words straightforward, but make sure to do it as a last step since lots of the substitutions we've done have created extra whitespace. Also the number of carriage returns (`\\n` characters) and the number of blank lines (`\\n\\n`) might be predictive so we'll count those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count carriage returs and blank lines\n",
    "nblanks, nnewlines = body.count(\"\\n\\n\"), body.count(\"\\n\")\n",
    "\n",
    "# Make all white space a single space\n",
    "regx = re.compile(r\"\\s+\")\n",
    "body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "# Remove any trailing or leading white space\n",
    "body = body.strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello as seen on nbc cbs and cnn and even oprah the health discovery that actually reverses aging while burning fat without dieting or exercise this proven discovery has even been reported on by the new england journal of medicine forget aging and dieting forever and its guaranteed reduce body fat and build lean muscle without exercise enhace sexual performance remove wrinkles and cellulite lower blood pressure and improve cholesterol profile improve sleep vision and memory restore hair color and growth strengthen the immune system increase energy and cardiac output turn back your bodys biological time clock years in months of usage for free information and get free month supply of hgh click here you are receiving this email as a subscriber to the opt in america mailing list to remove yourself from all related maillists just click here'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a true bag of words, so now we can get our word count to use in normalizing counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = len(body.split(\" \"))\n",
    "nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words with `nltk`\n",
    "Each email is going to have lots of words which are the \"glue\" of the english language but don't carry much semantic weight in determining the real topic or tone of an email. These are called [Stop Words](https://en.wikipedia.org/wiki/Stop_words) and we will go ahead and strip them out from the start. \n",
    "\n",
    "The Natural Language Tool Kit module (`ntlk`) includes a crap ton of functionality for processing text and also access to some public \"corpora\" such as for stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all useless stopwords\n",
    "bodywords = body.split(\" \")\n",
    "keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "body = \" \".join(keepwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello seen nbc cbs cnn even oprah health discovery actually reverses aging burning fat without dieting exercise proven discovery even reported new england journal medicine forget aging dieting forever guaranteed reduce body fat build lean muscle without exercise enhace sexual performance remove wrinkles cellulite lower blood pressure improve cholesterol profile improve sleep vision memory restore hair color growth strengthen immune system increase energy cardiac output turn back bodys biological time clock years months usage free information get free month supply hgh click receiving email subscriber opt america mailing list remove related maillists click'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with `nltk`\n",
    "This classifier is trying to determine the intent or tone of an email (spam vs. ham) by virtue of the specific words in that email, among other things. We don't expect that a slight variation on the same root word, like \"battery\" versus \"batteries\", carries much difference in intent or tone. Thus when we begin to represent our emails in the feature space of word content, we would do better to replace all the variants of each root with the root itself: this reduces the complexity of emails without really reducing the information about tone or intent. This process is called **stemming** and the `nltk` module has several options for out-of-the-box stemmers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem all words\n",
    "words = body.split(\" \")\n",
    "stemwords = [stemmer.stem(wd) for wd in words]\n",
    "body = \" \".join(stemwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello seen nbc cbs cnn even oprah health discoveri actual revers age burn fat without diet exercis proven discoveri even report new england journal medicin forget age diet forev guarante reduc bodi fat build lean muscl without exercis enhac sexual perform remov wrinkl cellulit lower blood pressur improv cholesterol profil improv sleep vision memori restor hair color growth strengthen immun system increas energi cardiac output turn back bodi biolog time clock year month usag free inform get free month suppli hgh click receiv email subscrib opt america mail list remov relat maillist click'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulate Preprocessing in a Function\n",
    "All of the above steps can be combined to a function that spits out the final processed word salad with the other features of interest that we exracted along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_salad(body):\n",
    "    '''Produce a word salad and some useful features from email body.'''\n",
    "\n",
    "    # Parse HTML extract content only (but count tags)\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    nhtml = len(soup.find_all())\n",
    "    nlinks = len(soup.find_all(\"a\"))\n",
    "    body = soup.get_text()\n",
    "    \n",
    "    # Replace and count all URLs \n",
    "    regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "    body, nhttps = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Replace and count all email addresses\n",
    "    regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "    body, nemails = regx.subn(repl=\" \", string=body)\n",
    "    \n",
    "    # Count uppercases then lowercase everything\n",
    "    nupper = len([charup for charup, char in zip(body, body.lower()) if charup != char])\n",
    "    body = body.lower()\n",
    "    \n",
    "    # Count and replace all numbers (integer and float)\n",
    "    regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "    body, nnum = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Count and replace all dollar signs\n",
    "    regx = re.compile(r\"[$]\")\n",
    "    body, ndollar = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Count number of special punctuation\n",
    "    nexclaim, nquest = body.count(\"!\"), body.count(\"?\")\n",
    "\n",
    "    # Remove all other punctuation (dashes replace with space)\n",
    "    regx = re.compile(r\"[^\\w\\s_-]+\")  \n",
    "    body = regx.sub(repl=\"\", string=body)\n",
    "    regx = re.compile(r\"[_-]+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "    \n",
    "    # Count carriage returs and blank lines\n",
    "    nblanks, nnewlines = body.count(\"\\n\\n\"), body.count(\"\\n\")\n",
    "\n",
    "    # Make all white space a single space\n",
    "    regx = re.compile(r\"\\s+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "    # Remove any trailing or leading white space\n",
    "    body = body.strip(\" \")\n",
    "    \n",
    "    # Get total word count\n",
    "    nwords = len(body.split(\" \"))\n",
    "    freqns = {\"email\": nemails/nwords, \"http\":nhttps/nwords,\n",
    "              \"exclaim\":nexclaim/nwords, \"quest\":nquest/nwords, \n",
    "              \"dollar\":ndollar/nwords, \n",
    "              \"blank\":nblanks/nwords, \"newline\":nnewlines/nwords, \n",
    "              \"html\":nhtml/nwords, \"link\":nlinks/nwords}\n",
    " \n",
    "    # Remove all useless stopwords\n",
    "    bodywords = body.split(\" \")\n",
    "    keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "\n",
    "    # Stem all words\n",
    "    stemwords = [stemmer.stem(wd) for wd in keepwords]\n",
    "    body = \" \".join(stemwords)\n",
    "\n",
    "    return freqns, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blank': 0.02877697841726619,\n",
       " 'dollar': 0.0,\n",
       " 'email': 0.007194244604316547,\n",
       " 'exclaim': 0.05755395683453238,\n",
       " 'html': 0.30935251798561153,\n",
       " 'http': 0.0,\n",
       " 'link': 0.014388489208633094,\n",
       " 'newline': 0.2158273381294964,\n",
       " 'quest': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out our functions\n",
    "body = get_body(spamfiles[179])\n",
    "freqns, body = word_salad(body)\n",
    "freqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello seen nbc cbs cnn even oprah health discoveri actual revers age burn fat without diet exercis proven discoveri even report new england journal medicin forget age diet forev guarante reduc bodi fat build lean muscl without exercis enhac sexual perform remov wrinkl cellulit lower blood pressur improv cholesterol profil improv sleep vision memori restor hair color growth strengthen immun system increas energi cardiac output turn back bodi biolog time clock year month usag free inform get free month suppli hgh click receiv email subscrib opt america mail list remov relat maillist click'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Corpus of Processed Emails\n",
    "Whatever algorithm we ultimately use for classification will require numeric feature vectors, so mapping each word salad to such a vector is the next main task. We'll start by building a corpus that is just a list of fully processed emails, and we'll build alongside it a dataframe of the other features of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails =  [\"email\"]*len(hamfiles + spamfiles)  # Reserve in memory, faster than append\n",
    "fnames = [os.path.split(fpath)[1] for fpath in hamfiles + spamfiles]\n",
    "df = pd.DataFrame(columns = [\"email\", \"http\", \"blank\", \"dollar\", \n",
    "                             \"exclaim\", \"html\", \"link\", \"newline\", \"quest\"], index=fnames)\n",
    "y = [0]*len(hamfiles) + [1]*len(spamfiles)  # Ground truth vector\n",
    "\n",
    "for idx, fpath in enumerate(hamfiles + spamfiles):\n",
    "    body = get_body(fpath)  # Extract only the email body text\n",
    "    freqns, body = word_salad(body)  # All preprocessing\n",
    "    fname = os.path.split(fpath)[1]\n",
    "    emails[idx] = body\n",
    "    df.loc[fname] = freqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>http</th>\n",
       "      <th>blank</th>\n",
       "      <th>dollar</th>\n",
       "      <th>exclaim</th>\n",
       "      <th>html</th>\n",
       "      <th>link</th>\n",
       "      <th>newline</th>\n",
       "      <th>quest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001.ea7e79d3153e7469e7a9c3e0af6a357e</th>\n",
       "      <td>0.00980392</td>\n",
       "      <td>0.00490196</td>\n",
       "      <td>0.0784314</td>\n",
       "      <td>0.0147059</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00490196</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.b3120c4bcbf3101e661161ee7efcb8bf</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003.acfc5ad94bbd27118a0d8685d18c89dd</th>\n",
       "      <td>0.00411523</td>\n",
       "      <td>0.00823045</td>\n",
       "      <td>0.0288066</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00823045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004.e8d5727378ddde5c3be181df593f1712</th>\n",
       "      <td>0.00645161</td>\n",
       "      <td>0.0129032</td>\n",
       "      <td>0.0451613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005.8c3b9e9c0f3f183ddaf7592a11b99957</th>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0.0483092</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198068</td>\n",
       "      <td>0.00966184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            email        http      blank  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e  0.00980392  0.00490196  0.0784314   \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf        0.01        0.02       0.06   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd  0.00411523  0.00823045  0.0288066   \n",
       "0004.e8d5727378ddde5c3be181df593f1712  0.00645161   0.0129032  0.0451613   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957  0.00483092  0.00483092  0.0483092   \n",
       "\n",
       "                                          dollar     exclaim        html link  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e  0.0147059           0  0.00490196    0   \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf          0        0.02           0    0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd          0  0.00823045           0    0   \n",
       "0004.e8d5727378ddde5c3be181df593f1712          0           0           0    0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957          0  0.00483092           0    0   \n",
       "\n",
       "                                        newline       quest  \n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e      0.25           0  \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf      0.27        0.01  \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd  0.160494           0  \n",
       "0004.e8d5727378ddde5c3be181df593f1712  0.212903           0  \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957  0.198068  0.00966184  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date wed aug chris garrigu messag id cant reproduc error repeat like everi time without fail debug log pick happen pick exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri ftoc pickmsg hit mark hit tkerror syntax error express int note run pick command hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri hit that hit come obvious version nmh im use delta pick version pick nmh compil fuchsia cs mu oz au sun mar ict relev part mh profil delta mhparam pick seq sel list sinc pick command work sequenc actual one that explicit command line search popup one come mh profil get creat kre ps still use version code form day ago havent abl reach cvs repositori today local rout issu think exmh worker mail list'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle these objects for easier access later\n",
    "with open(\"easyham_and_spam_corpus_and_df_and_y.pickle\", \"wb\") as myfile:\n",
    "    pickle.dump([emails, df, y], myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now in position to start mapping emails into a numeric vector space. It turns out there are a lot of ways in which to do this and the proper ML approach would be to search over this space using cross-validation to identify the best approach. This is the subject of Spam Part II. We'll explore different vectorization schemes and feed these vectors into a Support Vector Machine to classify each email. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a \"Vocab List\" of Selected Words\n",
    "If we take every email word salad in the data set and combine all the words into a single list then it will have a very large number of unique elements, but some of those words don't appear very often and thus won't be very useful for us. The list of relatively frequent words that will define our feature space is called our \"Vocab List\" and can be used to map each email into a numeric vector. Let's explore what our vocab list might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatlist = [word for email in emails for word in email.split(\" \") ]  # A flat list of all words from all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count number of appearances of each word-frequency value\n",
    "import collections\n",
    "counts_dict = collections.Counter(flatlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get a sense of the distribution over word-frequencies. The word \"number\" is going to be vastly more common than any other word and definitely any word that only appears once or twice in the whole email corpus is not going to help us much, so we'll ignore those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAElCAYAAAA7s++HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcHFW5//HPNwthCSogEIOEsIiyKCJXERRBcAGNgBKF\nK4q4whUFBVx+iCSg6FW8IlzRCwioLCqbiIC4IAPiCiKgoBL2oGQDRAIkQPL8/jinmUqnu6d7pqe6\np/v7fr361d2nTlU9Vd3Tz5xTp6oUEZiZmZVpXKcDMDOz/uPkY2ZmpXPyMTOz0jn5mJlZ6Zx8zMys\ndE4+ZmZWOicfs0zSepKulfSIpBM6HU8/k/Q+ST8a5XXMlPTt0VyH1Sef59P7JN0DrAc8DQgIYPOI\nmNfJuLqNpKOBl0bEzBrTrgB2Iu27VfPz0jz5nIj4cGmBpnjeCFwBPMbgZ/qTiNi3zDjGMkkCbgP2\njIg5nY6n30zodABWigDeHBFXN6okaXxELCsppm60EenHaCUR8abKa0lnAXMj4pjhrKSN+/nOiNi8\nxPX1lIgISRcAHwQ+2el4+o273fqHViqQNpK0PHdx3AtclctfKenXkh6W9CdJOxfmmS5pIHdN/VTS\n/0o6O0/bWdLcqnXcLWnX/FqSPi3pDkkLJX1f0nOqYjlA0r2SFkg6qrCccZKOyvP+W9L1kjaQ9HVJ\nX6la548kHVZzJ0g7SvpD3rbfS9ohl58FvAf4VF7+ri3vYOmtkm7Oy75G0haFaQ9IOkLSX4BHCmUf\nl/SXvM5TJE2R9LO8f6+QNHkYcXxR0rl5/z4C7Jv332cl3Zn37dmSnlWY5/15v8+XdGSObcc87XtV\nn8UbJc0pvH++pEvyZ3qHpIOqYjlH0nl5G2+S9JLC9I0K8y6odHdKOkjSzwv1tpZ0laSHJN0qaa/C\ntL0k/TUv/15JH2lhdw0AM1qob+0SEX70+AO4G9i1RvlGwHLg28BqwCRgKrAIeGOus1t+v05+/xvg\nBGAiqRvq38B387SdgfvqrRs4LM//vDz/N4HzqmI5FVgFeAmwBHhhnv4J4GZgs/z+xcBawMuB+wvr\nWwdYDDy3xvauBTwEvJP0j9d++f1aefpZwHFN7M+V6gGvBP4BvJSU6D8A/B0Yl6c/APweWB+YVCi7\nJsf1/BzL74At82fxK+ATdWJ4I3B7nWlfBJ4ofIaTgE/lda2f9++ZwJl5+rb5c3xF/ly+DjwJ7Jin\nfw84qta68368BTgSGA9sBtwD7FSIZTGwa94vXwWuztMmkFqax5O6MicBr8zTDgJ+ll+vCfwT2C+/\n3w54ENgkv18EbFf4jLeps182y/v4uYWy5wHLgImd/jvtt4dbPv3jkvxf40OSLi6UBzArIp6IiKXA\nu4DLI+KnABFxFXAD8CZJGwL/ARwTEU9FxK+AH7cQw0HAZyLigYh4CjgOmCmp8j0MYHZEPBkRt5CS\nzTZ52vvzvHfkuP4cEQ9HxPXAI5J2y/X2AwYiYlGN9b+Z9KN5XkQsj4jvA38D3tLCNtTzIeDrEXFT\nJN8i/ZhuV6jz1YiYn/dzxYl5O+4nJeZfR8Rtuc6PSImhnk3y5/lwfi7+B39N4TNcStr3n87rfxL4\nHFA5PjQTuDAi/pA/l6NIiaQZO5GS6VciYln+fL5N+hwqfhkRv4yIAM5m8DPdCVgzIj4TEUsiYmlE\n/K7GOt4K/Dl/XkTEH0nfu33y9KeBrSVNzvvy5lqBRsQdEbF21Xfj0fz87Ca319rEx3z6x15R/5jP\n/YXXGwHvkFT5QRbpe/JLUqvo4Yh4olD/XtJ/7c3YCPihpOWFZT9F+m+8Yn7h9eNApdtpQ+CuOsv9\nLilpXpWfv1an3tQcb9G9wAbNBD+EjYC3S/pEfi9SK2ID4Ppcdn+N+RYUXj/Bitv/BIPbX8tdUf+Y\nz9yq9xsCV0iqjDASgKS1SfvlvkrFiPh37q5rxjRgY0kPFZY7Dvh5oU5xYEvxM30+qWU8lI2AnavW\nMZ7UigHYCzgaOFHSjaQke0OT8a+Zn5vdXmsTJ5/+sdIxn4LikMe5pG60g6orSZoGrCVptUICmkbq\nLoM08mr1Qv3xwLqFRdwHvC8ifltj2RsNEf9cYFNqDwg4B/hzPpbwIuCSOsv4J4P/LVdMA34yxLqb\nMRe4LCJObFCnzKGl1eu6H3hbRPypuqKkB0jJqfL+2azYEljhcyV1VVXMBf4aEdvQurnA9Cbr/TQi\n9qo1MSJ+D7xF0gTgCOA8YMiBGNkWwN9yi89K5G43q05K55D+kN+QD1KvqjSQYGpE3EfqgjtW0kRJ\nr2bFLqvbgVUl7ZF/CI4mHV+oOBX4Qk5iSFpX0p4NYin6FvA5SZvleV8saS2AiPhHjuts4KKqbq2i\nK4AXSNpP0nhJ+5J+fC5rsN5mnQZ8VNJ2Ob7Jkt4iadU2LLsdTgW+JOn58Mw5TZVuuvOBt0l6uaRV\ngM+TjoNU3ATMkPRsSRsAxQP61+XlHSZpkqQJ+bNp1F2owryPSvqcpNXyd22HGvUvAbaV9I68/FUk\nbS/pBZJWl7SvpDVzzIurYh/KzrTnnw9rUUeTjwZHONV7nJnrTZA0S2mkzlJJcyWdqGGMBOpTjf7j\nXmFaPvawF6nffyGpW+pIBr8r+5MOrj8IfBb4TmHefwMfBs4g/af9KCt2NZ1EOo7xs9yt8xvSQe56\ncRbff5X0I1mZ91ukQRIV3wG2JnXB1d7QiIdII5uOJB2kPpI0BL3SfdNsy2SlehHxG+BQ4FRJD5OO\nJf1noW6tZTfa3nb7Eqkr7Jd5/11HPp4UETeRWgwXkVoZ95D2T8WZwJ2k78KlpJYFed6ngTcBO+bp\n84FvAGs0iCWq5n0p6XtyL7D3SpUj/kUa5PBe0iCN+0nHrCo9N+/LMT9M6nY9oNZKc7L6t6TnFor3\nJf3jYCXr6EmmkqYAF1cVTyb9iATwxYg4Wmko7/6k/2jmAJuQ/qMeiIiWh8Ra+0iaBWwaETX/4EuM\nYyfg7IiY3sk4ekXuitsnJ9WeJGkmMCMiDux0LP2ooy2fiJgXETsWHwwmo6eAb+bm+/6kZHRoRGxJ\nGp0D6SDkSv8pWX+RNJE0jPv0TsdiY0dEXOjE0zlddcxH0mrAIaREc27uy9+jUKWSmC4nnQMCsHt5\nEVq3kfQiUnfL+qRuPWsPX3fLRlW3jXb7IOkkweWkExmhMAqHPCw1IkLSItIw1mmlRmgriIhjO7z+\nv9F4OLINQ0RM7XQM1tu6JvnkYbkfI/3H9eP8o9JwlgbL8n9tZmbDFBGNRp62RTd1u+3L4Jj/LxfK\niyfLrQfPXI12nVx2H2ZmNqZ0U/L5BKnVc13VSYhXFl5XThCcQboWVPX0FYz2tYnKfsyaNavjMXTj\nw/vF+8b7pT2PMnVF8lG6N0nlDOkvFadFxI2kCxsCnCzpNuBCUqK6NiLqnc1uZmZdqluO+VRaPbdG\nxBU1ph9AOnv+ANI5PguBC0gnOZqZ2RjTFcknIl43xPRlwLH50bd22WWXTofQlbxf6vO+qc37pfN6\n8jbaldFuvbhtZmajJY3l6r/RbmZm1iecfMzMrHROPmZmVrqeTz5TpkxnypTpnQ7DzMwKen7AQeEA\nWkdjMjPrdh5wYGZmPc3Jx8zMSufkY2ZmpXPyMTOz0jn5mJlZ6Zx8zMysdE4+ZmZWOicfMzMrnZOP\nmZmVzsnHzMxK5+RjZmalc/IxM7PSOfmYmVnpnHzMzKx0Tj5mZla6rkg+ktaW9DVJd0laImmBpKsl\nbZOnT5A0S9KdkpZKmivpREmTOx27mZm1bkKnA5C0NvAHYBPgaeB2YBnwMmBT4GbgLGD/XD4n1z0M\n2AbYtfyozcxsJLqh5XM8KZncD2wREVtHxDbAc4ArJG1LSjwBHBoRWwIz87w7S9q7E0GbmdnwdUPy\neTspsdwFnC9psaRbgYMiYgmwR6Huxfn5cmBJfr17aZGamVlbdLTbTdK6wNqk5PMaYAEwH9gCOEXS\neGDDwiwLACIiJC0CNgCmlRq0mZmNWKeP+RTXv4jU/bYE+BXwSuAQ4Jo682qohc+ePfuZ1wMDA+yy\nyy7DDNPMrDcNDAwwMDBQ+noVEaWv9JmVSxOAxcBE4LcR8epcfjLwEVIi+hzpuFAAUyNiviQBjwGT\ngNMj4uCq5QZARJCqptdmZlZf4fdyyH/uR6qjx3wi4mlggNSK2VzS6pLGAdvmKn8HrizMsk9+ngGs\nml8Xp5uZ2RjQ0ZYPgKT/IHWzrQIsBB4HppNaOvtExCWSzgX2y2W3k4ZgTwCujYjX1limWz5mZi3q\nm5YPQETcAOwMXAWsDjwLuBp4bURckqsdABwH3Es6LrQQOBl4S+kBm5nZiHW85TMa3PIxM2tdX7V8\nzMys/zj5mJlZ6Zx8zMysdE4+ZmZWOicfMzMrnZOPmZmVzsnHzMxK5+RjZmalc/IxM7PSOfmYmVnp\nnHzMzKx0Tj5mZlY6Jx8zMyudk4+ZmZXOycfMzErn5GNmZqVz8jEzs9I5+ZiZWemcfMzMrHROPmZm\nVrqGyUfSFZJ2KbwfJ2lzSauOemRmZtazhmr57A5MLbxfC/grsGM7Vi5plqTlNR7LJI3LdSbkendK\nWipprqQTJU1uRwxmZla+CcOYR22PAhYCdxbeR34AnAXsDywD5gCbAIcB2wC7jkIsZmY2yrrlmM/l\nEbFj4fGqiAhJ25ISTwCHRsSWwMw8z86S9u5YxGZmNmzdknxmSnpc0j8lXSbppbl8j0Kdi/Pz5cCS\n/Hr30iI0M7O2aSb5RJNlw/U08ABwN7A+8CbgNzkBbViotwAgIgJYlMumNVpwqmpmZt2mmWM+Z0g6\ntarsMknLatSNiHh2C+s/FzgpIv4FIOn1wE+BScCHScd5amnquNPMmTOfeT0wMMAuu+zSQmhmZr1v\nYGCAgYGB0terRq0DSQO02MqJiNeOKCBpIbA28HPgGuD4HMPUiJgvScBjpAR1ekQcXGMZAXDKKadw\nyCGHVOIaSVhmZj0v/bxCRIzGwLIVNGz5RMQuo7lySR8DfhARD+T3rwfWISWbe0itoONz9X2AbwAz\ngFVznStHMz4zMxsdQ51keoykrUdx/YcB90u6R9KtpGQSpJbNiRFxI/C9XPdkSbcBF+Y610bEJaMY\nm5mZjZKhBhzMBl4yius/HvgFqQW2Mam1cw6wXUT8Pdc5ADgOuJd0js9C4GTgLaMYl5mZjaLhnGTa\nNhHxLeBbQ9RZBhybH2Zm1gO65TwfMzPrI8M9z8fMzGzYmul2O1rSB5tcXkTEbiMJyMzMel8zyWeL\n/GiGW0lmZjakZrrd3hUR45p8jB/1iM3MbMzzgAMzMyudk4+ZmZXOycfMzEo3VPL5DiveYdTMzGzE\nhrqw6HvLCsTMzPpHX3W7TZkynSlTpnc6DDOzvtfRa7uVbf78ezsdgpmZ0WctHzMz6w5OPmZmVrq6\nyUfSMknvLLw/U9L25YRlZma9rFHLZzlQvFzOgcCmoxqNmZn1hUbJ5z5gp6oyXzjUzMxGrNFot7OB\nYyTNBP6Vy74m6fgG80REuHVkZmYNNUo+xwL3Aq8DpgAbAQ8C80uIy8zMeljd5BMRAZyVH0haDnw+\nIs4rKTYzM+tRrZxk+lrgttEKxMzM+kfT5/lExDURsVDJyyTNzI+XSVI7gpF0vqTl+XF+oXyCpFmS\n7pS0VNJcSSdKmtyO9ZqZWblauryOpN2Bb5CO/xTdI+nDEfHT4QYi6b3ATGqPqDsL2B9YBswBNgEO\nA7YBdh3uOs3MrDOabvlIehVwKbAWcBLwofw4KZddKmnH4QQhadO8nN8A/6iati0p8QRwaERsSUpS\nADtL2ns46zQzs85ppeVzDDAP2D4iHihOkHQC8PtcZ/dWApA0HjgXeJqUZAaqquxReH1xfr4cWAJM\nyuu7pJV1mplZZ7VybbftgdOqEw9ALjsdeOUwYpgNvBz4cETUuuz0hoXXC/L6AliUy6YNY51mZtZB\nrbR8VgEebTD937lO0yRtB3waOCcivt/KvMCQgxwuv/zyZ14PDAy0uHgzs943MDDQkd9HpUZEExWl\nPwJPAjtFxNNV0yYA1wKTImK7plcuvYc0mGAJaTABwOr5eXku/zLphNcApkbE/Dy67jFSt9vpEXFw\n1XID4JRTTuGQQw4BICKoDMprdpvNzPpJ4TeyLSOYG2ml2+2bpK63qyS9WdLG+TEDuCpP+8YwYghS\nElmdwcRTiW014LJC2T75eQawan595TDWaWZmHdR0ywdA0peAI+tMPiEiPj3igKS7SUO5L4yId+Sy\nc4H9SInqdtLVtScA10bEa2ssIwDWXHNdHn10IeCWj5nZUMps+bR0nk9EfErSGcBewMa5+C7g0oi4\nvY1xBSue73MAKekcQDrHZyFwAfDZRgupJB4zM+suLbV8xopKy6fILR8zs8a69ZiPmZlZWzj5mJlZ\n6Zx8zMysdE4+ZmZWOicfMzMrnZOPmZmVrpVbKqwjaYuqso0l/a+kcyW9sf3hmZlZL2rl2m7nAJtH\nxCvy+8nA34CpucpyYNeIuHY0Am2Fz/MxM2tdt57nswNwReH9vqTE86b8/Ffgk+0LzczMelUryWd9\nYG7h/R7ADRFxZUTMA74NbNvG2MzMrEe1knyeIl1lumJn4JrC+38B67QjqDJMmTKdKVOmM3v27E6H\nYmbWd1o55vN74HFgV+AtwA+BN0fElXn6bOBDETG17kJK0swxn8rrynszs37XrVe1PoXUtfYw6b47\nd5Hu41OxE/DntkVmZmY9q+nkExHfzS2KvYFHgC9ExFOQhmEDz2F4N5PrelOmTAdg3rx7OhqHmVmv\n6NtbKrTS7eYh2mbWD7pyqLWkuyTt2WD6DEl3tScsMzPrZa2MdpsOTG4wfQ3S7a/NzMwaaue13dYn\njYYzMzNrqOGAA0mvAXYpFL1N0mY1qq4N7Afc1L7QzMysVzUccCBpFjArvw2g0UGoO4B3RsQN7Qtv\neDzgwMysdWUOOBgq+TybNIRapPN6Pgb8qKpaAIsj4qHRCrJVKyefSay//hTmz78XWDn5rL9+OlRV\nbyi1k4+Z9YOuST4rVJR2Bv4aEQvaGoD0fuBgYBPSgIZFwJ+A/46I63KdCcBngAOA5wMLgAuBz0bE\n4hrLbLhR1cmnWF4nxobTzcx6QVcmn1ELQDqTdJHSB0gDILYAJgJPAFtExH2Szgb2B5YBc0iJahVg\nICJ2rbFMJx8zsxZ1bfKRNA04CHgB6SKi1QFGROzWUgDSKhHxZOH9+4BvkbrzZgL3AH/M7z8SEd+U\nNAO4NJftExGXVC2zwUat2AVXFXy9GBtONzPrBV15bTdJe5AuJroKsBh4sB0BRMSTknYCTiBdM+6F\nedITwA3AuwvVL87PlwNLgEnA7sAKyaexpTUTj5mZlaeVC4t+kXQ8Zu9RGNG2NvDywvv5wMyImCtp\nw0L5AkjNK0mLgA2AaW2OxczMRlkryedFwNGjMZQ6In4EjJe0HmlgwUeB8yS9usFsLTYLJ5JuSVRt\nErC0tUWZmfWIgYEBBgYGSl9vK6Pd5gInRMTJoxqQtCbpqtlBam09Dnw+v58aEfOVOiYfI2WO0yPi\n4KplDOvgjI/5mFk/68oLiwJnA/u0c+WSVpP0AUmrFoqLFy9dA7iy8L6y/hlAZZ7idDMzGwNaafls\nDnyHdNzlJOBu0tDnFUTEfU2vPJ3E+jCp3+tOUktm0zz5SeBVEfFHSeeSLt8TwO25zgTg2oh4bY3l\ntqXlU7mPT/HkVDOzXtWVQ60lLWfwEjt1Z4qI8U2vXJoEnAa8EphKGkm3EPgNqYvv+lxvPHA06STT\nDUgDHy5gmCeZNoi9ejkNp5uZ9ZJuTT6zaZB0KiLi2BHGNGJOPmZmrevK5DOWOPmYmbWuWwcc9JBJ\nnQ7AzKyvtXKFg9c0Uy8irh1+OGXxeT1mZp00nAEHDbUy4GC0uNvNzKx1XXltN+C9debfFDiQdAHQ\nU0cekpmZ9bqmk09EfKfeNEknADe2JSIzM+t5bRlwEBEPk26D8Ml2LM/MzHpbO0e7PUy6ydsY4lFv\nZmad0Jbkk6/N9m5gXjuWVx6PejMz64RWhlqfWWfS2sAOwLrAJ9oRlJmZ9bZWRrsdWKf8IdLFPj8e\nEeeNOKIuMXv27E6HYGbWs3x5nYLivqg+x6d6uplZr/HldczMrKe10u0GgKRnAa9jcGTbXcDPI+LR\ndgbWjSr395k3756OxmFmNta11O0m6QPA/wCTSff1gXTJncXA4RFxRtsjHIbR6narVc/MrFd05S0V\nJO0JXEJq6ZwM3JonbQV8lNQS2jsifjwKcbbEycfMrHXdmnyuA9YCtq++e6ikNYHfAQ9HxKvbHmWL\nnHzMzFrXrQMOtgG+Xeu21fl4z3dyHTMzs4ZaST5DZUI3B8zMrCmtJJ+bgQMlrVE9QdJk0kmoN7cp\nLjMz62GtDLU+AbgYuFHSycBtubwy4GAz4G3tDa9cHkptZlaOVodafxj4ErAGg91sAh4DPhkR32xp\n5dIRwJuAFwLPBRYCvwWOi4i/5DoTgM8ABwDPBxYAFwKfrXX8Kc8zoi7AiPCAAzPrO1052u2ZGaTn\nAK8HNs5FlZNMH2l55dLdwDTgDmA5sDkpmS0Gto6I+ySdDewPLAPmkIZ0rwIMRMSudZbr5GNm1qKu\nTj5tXbl0NHBuRNyd33+cdBJrAIcD1wJ/zO8/EhHflDQDuDSX7RMRl9RY7jA2ahKVWyw4+ZhZP+qa\nodaSxkv6b0kHD1HvvyR9UVJL14qLiM9XEk/288LrpcAehfcX5+fLgSX59e6trK8x39vHzKwsQyWL\nd5Hu0XP9EPX+QLqF9jtHGM8R+XkRcAGwYWHaAoBIzY5FuWzaCNdnZmYdMNRot3cAv4iIPzaqFBF/\nlPRT0rGZc1oNQtJE4AxSsnuEdJmeBxt0fY16k9DMrB8MDAwwMDBQ+nobHvORNA/4n4g4YcgFSZ8A\njoiIKS0FIK1Dumbcq4B/AG+OiFvytKOAz5OO70yNiPlKGekx0kGa0yNipS5BDzgwM2td1xzzId0i\ne0GTy1pIuvZb0yRtQeqyexXwJ9J1424pVLmy8Hqf/DwDWLXGdDMzGyOGSj6Pks6/acY6pCHSrfgh\ng0O2JwIXSfptfrwvIm4EvpennyzpNtI5PgFcW2uk2+iZtFLJlCnTnzkx1czMmjfUMZ9bgTeQhj8P\n5fUM3mahWasweLLqVlXTfpKfDwBuz8+bkFpYFwCfbXFdI7TyaLj58+8tNwQzsx4x1DGfj5ESz9si\n4kcN6u1JasUcHhEntT3KFpV1zKfQPzqS1ZmZdYWuOclU0mrATcB04CukA/z3FKZPBz4AHAncDWwb\nEUuql1O20U4+66+/ETDY8nHyMbNe0DXJJwezGXAZ6dI3AfybdCxoTeBZpGHPfwdmRMSdoxptk0Y7\n+dSqb2Y21nVV8skBrQp8EJhJOjbzLFIS+gtwEfCtiHhiFONsiZOPmVnrui75jDVOPmZmreum83z6\n0CQPnzYzG2Vu+bRBL+5DM+s/bvmYmVlPc/IZtsErHozkSge+SoKZ9SN3u7XRcPalT1Q1s27hbjcz\nM+tpTj4d5O42M+tX7nZro1b3ZfFcol78HMxsbHG3m5mZ9TQnn5om1Xk9Mh7ZZmaWuNutjZq4SOsK\n9dztZmbdxN1uZmbW05x8Wta+bjgzs37l5NOylW+nbWZmrXHyMTOz0jn5mJlZ6SZ0OoDuNYl2dbF5\neLWZ2Yo63vKRtJOkyyTNk7Q8P46pqjNB0ixJd0paKmmupBMlTR69yNp3bGf+/HuZP//eti3PzGys\n63jyAV4GvAF4ML+vdcLLWcAsYBpwJ7AucBhwaRkBmplZe3VD8vku8CzgFbUmStoW2J+UlA6NiC2B\nmXnyzpL2LiXKJrTrCgazZ88e8TLMzLpZ11zhQNIawKOkJHNsRByXy48CPp/Lp0bEfKXTcB8jHZg5\nPSIOrlpWRzeqep8Wr2RQnF7vCgeSfMUDMyudr3Cwog0LrxcARPplXpTLppUekZmZjchYHu026pm5\nNZXRcZNW6HqbN++elepNmTK9qrxWWVJZVq1pZmYjNTAwwMDAQOnrdbfbKIuIlbrd6pUXu+Oqu+a6\n5XMys97Vr91u9Tb2ysLrffLzDGDVGtPNzGwM6HjykfRWSXcAN1WKgMMkzZF0dkTcCHwvTztZ0m3A\nhaSW0LURcUn5UTev9si1SUOOihute//4nkJm1g063u0m6T3AmXUmD0TEbpImAJ8BDgA2IA02uAD4\nbEQsrrHMMdlHVWsUXPW0kXI3npnVU2a3W8eTz2hw8qnPycfM6unXYz49qPXbcdfrEqvXXTZlynTG\nj1/jmWnuVjOzscAtny5Sb2RcdZ2i6hNVh2rZuOVjZvW45WNmZj3NyWdUNNPFVl2n0Qi4Sc88F7vY\nzMzGKne7jVH1rg/nbjczGy53u5mZWU9z8jEzs9I5+fSwyrDrZoZfV+r4XkJmVgYf8xmjmjnmU+vC\npfWO+dS7t5CZ9Q8f8zEzs57m5DNizV25oLVlVT+vrHJlg0bThxtDs/M26s7rhSst9MI2mHUrd7v1\nkKGukNBst1ut6bU0GrbdC0O6e2EbzFrhbjczM+tpTj6jrp3dciNT7EKqdNuteMWE0Y11qG4sj7Qz\n6x/udushzVyYtJVlDaXVbrdmrr7QTd9Hd7tZv3G3m5mZ9TQnnzGpVvfYpMLot6HuIzRU99qklbrk\nJk9+7jPvq0fatToqrNi91sy8lfrFE2ZX7jIc1OjeR9Vdj9X3Qaq+P1Iro/96lUf9laPf9rO73WxI\nzXbnDdXFVutE1uqTYut14TWKod7IvaHKG52QO9Q6+om7H8vRDfvZ3W5mZtbTnHxsCJOqTmZtZUTc\npBW6tQbnH+zWq9aoy6ye6u6zYnllPSM7Ibdx19vKXXWjr5Xr9o10PcNddye6kZrtcm2XRiM0+60b\nrVVjqttN0n7AJ4AtgCeAXwKfjog7q+qNnY3qIY262Jrp1qp3fblmR/C12j3Y6sjAoe6R1KhOuw3n\npOCRrqfefuume0c12+XazvW1a/vd7dalJL0fOA94KfBPUuz7ANdJWq+TsZmZWWvGRPKRNBH4IhDA\nhRGxGbCEB/gQAAANj0lEQVQl8CiwHnBUB8Mz61oDAwOdDqEreb903oROB9CklwPPJSWfiwEi4gFJ\nvwNeD+zewdjGiEnA0mFMa96UKdNZuHAh66677grLbnS8pVJnxb7xNM+Ky6k/b4q91rGp2ttUva7m\ntn0wxoULFwLUiC/VmTfvnmfWU6xbLC9avHgxTzzxxAr16s1ba/5K2eLF/2Ly5OcAPFN/YGCAXXbZ\nZYV4inEffPCBzJ49e6X11VPrGEdl3YsX/6thjNUxtLKdxXmqpxfLkhU/h3rb0SgB1Vt2sbzRNray\nzFbVWs5Qx5aqv3sjjaEtIqLrH8C+wHJgGfDaQvl3c/njVfXDDz869ahotrxWvXrzNjN/sf6sWbOG\nnK+VZTazzup6jcpb3c5a01tZb/F9I7WWUV0+VJ16n3ur62ymXqvfySaWPeq/62Ol5VPPqB8UM2tV\nvYEMzQ5wqFWv1cERxfrHHntsy+sbTr1Wt3s429nKPEPVbWa7m1l2qzEPdz8Ot95wYijDmDjmA8wt\nvF6vxuv7SozFzMxGaKy0fK4HHgTWJo1w+4GkqcArSc3EnxQrRwnDBM3MbPjGzHk+kj4I/B+pq+1u\nYB3gWcAC4KURMa+D4ZmZWQvGSrcbEXE68C7gT8DzSAMNLgJe7cRjZja2jJmWj5mZ9Y4x0/LpJZKO\nkHSVpPslLZE0V9L5krYu1JkgaZakOyUtzXVOlDS5alnrSTpT0vy8rFslfbT8rWq/vE+W58f5hfK+\n3TeS1pb0NUl35W1aIOlqSdvk6X23byStLunLkv4uabGkRyTdIukoSeNynZ7fL5J2knSZpHmFv5tj\nquq0dT9I2k7SlXmfPybpOkmvayrgMsZz+7HSeUt3k85Z+jvw1/x6OfBvYFquc3Yuewq4DViS3/+y\nsJzVgb/l8sX5dWVZszu9nSPcR+9l8NyuZcD5hWl9uW9IA27uyNvwJPAX4GbgEeBt/bpvGDzfbxnw\n5/z3tTw/PtUv+wU4LH8vbi3sj2Oq6rRtPwAvztOXAfNJo44r383XDRlvp3dYPz6Ao4GNC+8/Xviy\nHAZsW3j/X7nOjELZ3rns8Fz2NLBVLvtKLlsCrNvpbR3m/tmUlIivy1/oZ5JPP+8b4Js5/vuATQvl\nAlbt133D4D9zP8nvJ5IS8jLgG/2yX4C18vdgDWokn3bvB+DSXHYnKWGNA36bl3XTUPG6260DIuLz\nEXF3oejnhddLgT0K7y/Oz5eTPngYvJxQ5XlORNyaX1+UnycCu7Un4vJIGg+cS/ri70/6Ihf17b4B\n3k46teAu4PzcxXQrcFBELKF/9821+fmNkv4CzAHWBH5PuiZkX+yXiHg4fw/qadt+yH+nu5G+jz+L\niMcjYjkpIQl4saQpjeJ18ukOR+TnRcAFwIaFaQsgX+8iTQeYlp83JH34Cwr15xdeT2PsmU26lt+H\nI+LeGtP7ct9IWpfU7QbwGmAD0vZsAZwi6RD6dN8AHwLOya+3IG3fk8AtpPMD+3W/VGvnfngusFpx\nWXXq1eXk00GSJkr6LvAeUjfB3hHxYKNZmllsW4LrAEnbAZ8GzomI77c6e5vqdKviCeGLgE2AF5C6\nOQAOaTBvr++bjwPvBn4HrA+8CHgY+CBwUoP5en2/NKud+6Hp/eXk0yGS1iHdDO9dwD+AnSOi8kOy\n0uWElC7GtE4uu69QT9S+5FCx3lixNTAeeLukRyU9yuB/T2/N7/9ZqN9P+2Yh6b95gNsL3Rx/JG3n\ndFbcpr7YN5JWA44j/ad+UUQsiog5wDWkbdyNPtwvdbTzd2UR6Yae1dOa3l9OPh0gaQvgD8CrSCfN\nbh8RtxSqXFl4vU9+nkE6mAiDlxOq1HtBYZj2zPz8FHBVO+MuSZDuc7B6flSMIzXzLyuU9c2+iYin\ngQHSj8LmeXjxONJBZEgjJ/vxe7M6g63ClwNIWoU0EgvgMfpvv9RrfbRtP0TEMtL+EPAGSWtImgDs\nmevdEkOd/N/pERr9+GBwGONyUr/0bwuP9+U655IOtj9NGhK5NL+/urCcNUg/OstIf2SV5S4Djuv0\ndrZpX1WGzRaHWvflvgH+g/Tf5jJgHmngQfVIpb7bN6SkXBkKfAepJ6GyPUf0y34B3pq3vzIcfznp\nmNcc4Ox27wfgJXn6MtJxn7kMDuN+/ZDxdnqH9eMj/2gsq/M4JtcZD8wiDWNcAtwPnAhMrlrW+sCZ\npAN9S/IX6qOd3sY27qvKMNofFMr6dt8ArwB+RhqKvoj03+dr+nnfAM8GvkA6Z+7R/IP7e+A9/bRf\nSMeO6/2uXJXrTGjnfgC2I7WUHsmJ6Fc0cY5PRPjyOmZmVj4f8zEzs9I5+ZiZWemcfMzMrHROPmZm\nVjonHzMzK52Tj5mZlc7Jx8zMSufkY2ZmpXPysbYp3Lp3qMcySWPm8vSS3jzEtmzZ6RjNxpoJQ1cx\na9q7qt7vRLrXymmky24ULSwlovY6k3Ql8mpza5SZWQNOPtY2EXFe8b2kiaTk89vqae0iaY2IeGw0\nll3DH4azHSXHaDYmuNvNOkrSOEkfk/QnSY9LekTSzyTtWFVvq9zNdbikd+f6T5Buk4ykC/M9gNaX\ndJ6kByX9S9IPJK2V6xwq6e+SnpD0F0lvaPO2bJNjPFTSgZJuzjF+rlBnmqQzJN0vaamk+ySdLOnZ\nNZa3naSrJT0maYGk0yVNz+v4aqHeXrlszxrLuETSwzXKt5Z0vqT5OY47JH0u345gpfklrSPpLEmL\n8ud0taSX1FjuuLz9Nyjd5vsRSTdK+mSefmCOdZ8a8yrvjxua2N02xrnlY512EfBm4PvAqaT7sxwI\nDEjaPSKqu7neDUwFvgl8nXQFY0j3ARoH/By4FTgK2JJ0h8+1Jf2adE+S00mXkz8c+KGkjSNiAc1Z\nQ+kmgEVLarRq3gtMAf6PdNXgRQCSXgRcR7pK8JnAvaTbPv8XsJOkHSJiSa67FelWAU8BXyF1U+6T\n91etqwHXu0JwVE+T9BrSvVvuJ93pcwHpdg2fzs97VM0/AbiadNXoo0n7/3Dgx5I2i4in8nIFXEK6\nR8w1wLGkq0xvBbwN+DLwA+BrwPvythS9EXg+cHydbbFe0unLgPvRuw8GL/F+QJ3p7ybd/2PfqvKJ\nwF9IN6SqlG2V6z4GTKuxrAvyur5QVX5qnu9vwKRC+Q65/P81sR1vZvB+JssLj2XAaYV62+TyR4EN\naiznGuAeYK2q8p3zfIcWyq4gJcltCmUCfpHX+9VC+V65bM8a6/wh8FDh/TjSLT2uBybW+bz2rJp/\nGfC5qrofyOVvL5R9KG/HyUPsz1NISXWDqvILgMXAmp3+7vox+g93u1kn7U+6X8gvcrfOOrll8Wzg\ncmArSVOq5rkwIhrdnvekqveVgQ5nRMTSSmGkW5Y/DbyghXhPAl5XeLwe+J8a9c6PiH8UCyQ9nzQA\n4wJgXNX23kq6Odwbct3V8rJ/ERE3F2IOUiuo3p0qm7Ej6ZbbZwPPqorjF6SWTq3uyK9Vvf9ljqO4\n//YnteqOGiKG00j31zmwUpDX/xbS5/tosxtjY5e73ayTtiDd873eyLcg3dSqeDveOQ2WtyQi5leV\nVY533FOj/iMM3r++GX+LlbsBa6kV4xb5+XDgiBrTK9sKMI304/z3GvVua2L9jVTi+BorJ5TqOCoe\ni4gHq8oq74v7bzPgrohY3CiAiLhZ0vWk7slKF9sBpBbvtxqHb73Cycc6ScB9pP7/ev/N31H1/vEG\ny1s2jGkjaUXUUyvGynpOBS6sM99w/+NvdEfI6r/xShyzScefaqk+BvZ0g+UPd/+dBpwm6bURcTXp\nOzAnIurFZD3Gycc6aQ6wPfCriGj0A9cLKq0hNdF6uo+ULF9UY9pWNcoeIiWBtWtM26ROHEubbMW1\n4nZge0mTh2r9AN8Dvgq8X9JjpO36VJvjsS7mYz7WSd8ljW47rtZESeu1aT0dv1d8RNwN/Bo4II9k\nW0EeZrxOrvsEadTebpJeWqgzjtRlV709d+Sy11Utcw8Gu9kqfkXqgvx4jeNpSJpYa9h3k84FVqWJ\n0WoR8TgpAb0NOJI0AOE7w1yvjUFu+dhoq9stExHfyT+Qn5K0A2n470PAhqSD82sDL603fztiKNn7\nSCPerpf0beAWYBVgU+CtpMEEJ+e6nwJeA1wt6eukgRkzgcnVC42IeZIuAv5T0lLg96Rh5vuTjhFN\nLdR9WtK7SPv6r5LOJI0EXBN4ISkZvB+4dBjbdyawN/ARSS8mjdh7NMfy8ojYsar+aaQRcvsAl0Tz\nQ96tBzj52Ghr2OqIiP0k/Yz0w/wZ0nfyAeAGVj4gvtI5K02uq+lzYIax7KaXGRFzJG0L/D/S8O33\nkoaO30dqBfy4UPfPknYGTiANUniMdA7N8cDdNRb/IeAJUhJ7B/A7UktoNvC8qjh+I+llOY6ZpAEG\nj+Tlfj3PW71NQ25rRFROdD2MNIDgWOBJ0tDuc2rsjxsl3QhsC5xRZx3Wo5RGb5rZWJC7xB4GvhYR\nh3c6npHKJ/9OI5275R+jPuJjPmbWEbn1tQPpHCwnnj7jbjczK5WkV5GOLx1B6u77emcjsk5wy8ds\n7GnlWFU3OpJ0vlOQLs+zqMPxWAf4mI+ZmZXOLR8zMyudk4+ZmZXOycfMzErn5GNmZqVz8jEzs9I5\n+ZiZWen+P/wtGJpEMkjmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1342cadb5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 4])\n",
    "snp.labs(\"Term Frequency\", \"Counts of TF\", \"Frequency of Term Frequencies :)\")\n",
    "plt.hist([num for num in counts_dict.values() if num > 50 and num < 12000], bins=1000)\n",
    "plt.xlim([50, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a sense of what our vocab list would be like if we only used words who appeared fifty or more times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist = [key for key in counts_dict if counts_dict[key] > 50]\n",
    "len(vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clue', 'ascii', 'remain', 'un', 'walk', 'default', 'market']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This criteria of demanding a frequency of at least 50 appearances would give us a vocab list of 1496 words to use to define our feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Emails to Numeric Vectors\n",
    "Given the corpus of processed emails and vocab list of selected words, **we want to map each email to a numeric vector that reflects the occurrence (or frequency) of different vocab words.** The tool in sklearn that helps us do this is `CountVectorizer`. It is a class that tokenizes input text and converts it into a numeric vector. Let's do an example using the vocab list we generated above and assuming we want our vectors to reflect binary occurrence of words from this vocab list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3051x1451 sparse matrix of type '<class 'numpy.int8'>'\n",
       "\twith 168443 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify our vocab list, and binary occurrence model\n",
    "vectorizer = CountVectorizer(vocabulary=vocablist, binary=True, dtype=np.int8)  \n",
    "X = vectorizer.transform(corpus)\n",
    "X  # notice dtype is int8 like we specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` is capable of creating a vocab list for you automatically, based on the criterion of *document* frequency (the number or proportion of documents that a word appears in). You can set the max and min frequency or proportion using kwargs `min_df` and `max_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-1232843f1995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Sonya\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    868\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sonya\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sonya\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(corpus, min_df=0.1, max_df=1.0, binary=True, dtype=np.int8)  \n",
    "X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Over Preprocessers with `Pipeline`\n",
    "Some uncertainty still remains in how we should proceed:\n",
    "\n",
    "- Our \"vocab list\" is the set words that we will use to generate the numeric vector so what words should be included in this list?\n",
    "- Should the numeric feature be binary occurrence (\"yes\" or \"no\" on whether a specific vocab word appears) or frequency counts (how many times a specific vocab word appeared)?\n",
    "- Should we include only individual words in our vocab list, or include groups of $n$ contiguous words (n-grams)? This would take us beyond the \"bag of words\" model because now the position of words in an email matters.\n",
    "\n",
    "All of the above questions have to do with *preprocessing* of the data into the numeric vectors rather than with the SVM algorithm itself. To compare the performance of an algorithm under different preprocessing options in sklearn we can use a `Pipeline`: a chain of sklearn objects which transform your data sequentially, ending with an estimator. Once the pipeline is created, `GridSearchCV` can be used to vary parameters not just of the final estimator, but of any step in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You initialize a pipeline by providing it with every object in the pipeline, each of which you give a nickname to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([(\"vect\", CountVectorizer()), (\"svm\", SVC())])\n",
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the vectorizer we will try binary vs. count vectorizing, n-grams of different sizes, and different frequency cut-offs for choosing the vocab list from the set of all words that appeared.\n",
    "\n",
    "Within the SVM we will want to optimize over the inverse regularization, `C`, as well as the spread parameter of the guassian kernel, `gamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Emails to Count Vectors\n",
    "Now that we have chosen the words that seem reasonably useful and created a Vocab List (sometimes referred to as a dictionary in NLP), we want to cast each email into a vector form, where each element of the vector should reflect the occurrence of a particular vocab word (so for us this will live in $R^{1451}$). This is a really common task in NLP and so there are some built in tools that can help us.\n",
    "\n",
    "The first question is whether our features should reflect just the presence (0 or 1) of the specific vocab word, if it is should somehow reflect the *frequency* of that word in the given email. There is a good discussion of this so called [*term-frequency* approach in the sklearn docs](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting). Interestingly you sometimes get better performance with the binary-occurrence feature approach when your data points are small pieces of text. We'll try this way first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
